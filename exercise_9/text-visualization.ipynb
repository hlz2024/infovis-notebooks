{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523005b4",
   "metadata": {},
   "source": [
    "# テキストデータの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748ec8f",
   "metadata": {},
   "source": [
    "[青空文庫](https://www.aozora.gr.jp/)に収録されている、夏目漱石の[『三四郎』](https://www.aozora.gr.jp/cards/000148/card794.html)という作品を使って、テキストデータの可視化を練習します。\n",
    "\n",
    "自然言語処理のフレームワークである[spaCy](https://spacy.io/)とこれを利用した日本語NLPライブラリである[GiNZA](https://megagonlabs.github.io/ginza/)を使います。\n",
    "\n",
    "![](https://spacy.io/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)\n",
    "\n",
    "spaCyでは、各コンポーネント（機能）が順番に適用されるpipeline方式でテキストが処理されます。基本的には、tokenizer（分かち書き）、tagger（品詞付与）、parser（係り受け解析）、ner（固有表現抽出）、lemmatizer（原形抽出）、textcat（文書分類）というコンポーネントが用意されています。\n",
    "\n",
    "`nlp()`を実行するとデフォルトでtokenizer、tagger、parser、ner、lemmatizerが入力文書に適用されます。\n",
    "\n",
    "spaCyとGiNZAと比べて、より長く使われてきたライブラリとして、\n",
    "\n",
    "* [NLTK (Natural Language Toolkit)](https://www.nltk.org/index.html)\n",
    "* [MeCab](https://taku910.github.io/mecab/)（形態素解析） + [CaboCha](https://taku910.github.io/cabocha/)（係り受け解析）\n",
    "* [JUMAN](https://nlp.ist.i.kyoto-u.ac.jp/?JUMAN)（形態素解析） + [KNP](https://nlp.ist.i.kyoto-u.ac.jp/?KNP)（係り受け解析）\n",
    "\n",
    "などがあります。\n",
    "\n",
    "より高度な分析をする時に必要となることがあるので、ぜひチェックしてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b0f94",
   "metadata": {},
   "source": [
    "それでは、spaCyを使った形態素解析の方法を確認してから、テキストデータの可視化を行いましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b9c80",
   "metadata": {},
   "source": [
    "## 形態素解析の練習\n",
    "\n",
    "形態素解析は、文章を一つ一つの形態素に分ける技術です。形態素は、「言葉が意味を持つまとまりの単語の最小単位」です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f7f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: ginza in /opt/anaconda3/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: ja-ginza in /opt/anaconda3/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: plac>=1.3.3 in /opt/anaconda3/lib/python3.11/site-packages (from ginza) (1.4.3)\n",
      "Requirement already satisfied: SudachiPy<0.7.0,>=0.6.2 in /opt/anaconda3/lib/python3.11/site-packages (from ginza) (0.6.9)\n",
      "Requirement already satisfied: SudachiDict-core>=20210802 in /opt/anaconda3/lib/python3.11/site-packages (from ginza) (20241021)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.2 which is incompatible.\n",
      "streamlit 1.30.0 requires numpy<2,>=1.19.3, but you have numpy 2.0.2 which is incompatible.\n",
      "pandas 2.1.4 requires numpy<2,>=1.23.2; python_version == \"3.11\", but you have numpy 2.0.2 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
      "numba 0.59.0 requires numpy<1.27,>=1.22, but you have numpy 2.0.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.0.2\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Collecting numpy>=1.17.3 (from scikit-learn)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "# ライブラリのインストール（初回のみ）\n",
    "\n",
    "!pip install spacy ginza ja-ginza\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ab651",
   "metadata": {},
   "source": [
    "### 日本語\n",
    "\n",
    "spaCyで日本語の形態素解析モデル（`ja_ginza`）をロードして、形態素解析をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4567bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これ\n",
      "は\n",
      "文章\n",
      "です\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "text = \"これは文章です。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ee04f",
   "metadata": {},
   "source": [
    "形態素解析の結果には、語の原形や品詞の情報も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d5d685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "うとうと\tうとうと\tADV\t副詞\n",
      "と\tと\tADP\t助詞-格助詞\n",
      "し\tする\tAUX\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "目\t目\tNOUN\t名詞-普通名詞-一般\n",
      "が\tが\tADP\t助詞-格助詞\n",
      "さめる\tさめる\tVERB\t動詞-一般\n",
      "と\tと\tSCONJ\t助詞-格助詞\n",
      "女\t女\tNOUN\t名詞-普通名詞-一般\n",
      "は\tは\tADP\t助詞-係助詞\n",
      "いつ\tいつ\tPRON\t代名詞\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "ま\tま\tNOUN\t名詞-普通名詞-助数詞可能\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "か\tか\tADP\t助詞-副助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "隣\t隣\tNOUN\t名詞-普通名詞-一般\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "じい\tじい\tNOUN\t名詞-普通名詞-一般\n",
      "さん\tさん\tNOUN\t接尾辞-名詞的-一般\n",
      "と\tと\tADP\t助詞-格助詞\n",
      "話\t話\tNOUN\t名詞-普通名詞-サ変可能\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "始め\t始める\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "いる\tいる\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n",
      "この\tこの\tDET\t連体詞\n",
      "じい\tじい\tNOUN\t名詞-普通名詞-一般\n",
      "さん\tさん\tNOUN\t接尾辞-名詞的-一般\n",
      "は\tは\tADP\t助詞-係助詞\n",
      "たしか\tたしか\tADJ\t形状詞-一般\n",
      "に\tだ\tAUX\t助動詞\n",
      "前\t前\tNOUN\t名詞-普通名詞-副詞可能\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "前\t前\tNOUN\t名詞-普通名詞-副詞可能\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "駅\t駅\tNOUN\t名詞-普通名詞-一般\n",
      "から\tから\tADP\t助詞-格助詞\n",
      "乗っ\t乗る\tVERB\t動詞-一般\n",
      "た\tた\tAUX\t助動詞\n",
      "いなか者\tいなか者\tPROPN\t名詞-普通名詞-一般\n",
      "で\tだ\tAUX\t助動詞\n",
      "ある\tある\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n",
      "発車\t発車\tNOUN\t名詞-普通名詞-サ変可能\n",
      "まぎわ\tまぎわ\tNOUN\t名詞-普通名詞-一般\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "頓狂\t頓狂\tVERB\t形状詞-一般\n",
      "な\tだ\tAUX\t助動詞\n",
      "声\t声\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "出し\t出す\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "駆け込ん\t駆け込む\tVERB\t動詞-一般\n",
      "で\tで\tSCONJ\t助詞-接続助詞\n",
      "来\t来る\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "いきなり\tいきなり\tADV\t副詞\n",
      "肌\t肌\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "ぬい\tぬぐ\tVERB\t動詞-一般\n",
      "だ\tだ\tAUX\t助動詞\n",
      "と\tと\tADP\t助詞-格助詞\n",
      "思っ\t思う\tVERB\t動詞-一般\n",
      "たら\tた\tAUX\t助動詞\n",
      "背中\t背中\tNOUN\t名詞-普通名詞-一般\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "お\tお\tNOUN\t接頭辞\n",
      "灸\t灸\tNOUN\t名詞-普通名詞-一般\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "あと\tあと\tNOUN\t名詞-普通名詞-副詞可能\n",
      "が\tが\tADP\t助詞-格助詞\n",
      "いっぱい\tいっぱい\tADV\t副詞\n",
      "あっ\tある\tVERB\t動詞-非自立可能\n",
      "た\tた\tAUX\t助動詞\n",
      "の\tの\tSCONJ\t助詞-準体助詞\n",
      "で\tだ\tAUX\t助動詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "三四郎\t三四郎\tPROPN\t名詞-固有名詞-人名-名\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "記憶\t記憶\tNOUN\t名詞-普通名詞-サ変可能\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "残っ\t残る\tVERB\t動詞-一般\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "いる\tいる\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n",
      "じい\tじい\tNOUN\t名詞-普通名詞-一般\n",
      "さん\tさん\tNOUN\t接尾辞-名詞的-一般\n",
      "が\tが\tADP\t助詞-格助詞\n",
      "汗\t汗\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "ふい\tふく\tVERB\t動詞-一般\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "肌\t肌\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "入れ\t入れる\tVERB\t動詞-一般\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "、\t、\tPUNCT\t補助記号-読点\n",
      "女\t女\tNOUN\t名詞-普通名詞-一般\n",
      "の\tの\tADP\t助詞-格助詞\n",
      "隣\t隣\tNOUN\t名詞-普通名詞-一般\n",
      "に\tに\tADP\t助詞-格助詞\n",
      "腰\t腰\tNOUN\t名詞-普通名詞-一般\n",
      "を\tを\tADP\t助詞-格助詞\n",
      "かけ\tかける\tVERB\t動詞-非自立可能\n",
      "た\tた\tAUX\t助動詞\n",
      "まで\tまで\tPART\t助詞-副助詞\n",
      "よく\tよく\tADV\t副詞\n",
      "注意\t注意\tVERB\t名詞-普通名詞-サ変可能\n",
      "し\tする\tAUX\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "見\t見る\tVERB\t動詞-非自立可能\n",
      "て\tて\tSCONJ\t助詞-接続助詞\n",
      "い\tいる\tVERB\t動詞-非自立可能\n",
      "た\tた\tAUX\t助動詞\n",
      "くらい\tくらい\tPART\t助詞-副助詞\n",
      "で\tだ\tAUX\t助動詞\n",
      "ある\tある\tVERB\t動詞-非自立可能\n",
      "。\t。\tPUNCT\t補助記号-句点\n"
     ]
    }
   ],
   "source": [
    "text = \"うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬいだと思ったら背中にお灸のあとがいっぱいあったので、三四郎の記憶に残っている。じいさんが汗をふいて、肌を入れて、女の隣に腰をかけたまでよく注意して見ていたくらいである。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5afef",
   "metadata": {},
   "source": [
    "### 英語\n",
    "\n",
    "spaCyでは、モデルを変えるだけで同じ手順で他言語の解析を行うことができます。\n",
    "\n",
    "英語のモデルでも試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3b858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# 英語のモデルをダウンロードする\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16440c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tthis\tPRON\tDT\n",
      "is\tbe\tAUX\tVBZ\n",
      "a\ta\tDET\tDT\n",
      "sentence\tsentence\tNOUN\tNN\n",
      ".\t.\tPUNCT\t.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"This is a sentence.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4c56e",
   "metadata": {},
   "source": [
    "## テキストデータの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865a216",
   "metadata": {},
   "source": [
    "それでは、『三四郎』を題材にテキストデータの可視化を行ってみましょう。\n",
    "\n",
    "* ワードクラウド\n",
    "* 共起ネットワーク\n",
    "* 共起マトリックス\n",
    "\n",
    "の3種類を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe05fba",
   "metadata": {},
   "source": [
    "### データの用意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88a8b0",
   "metadata": {},
   "source": [
    "まずは、青空文庫からテキストデータをダウンロードします。解凍されたファイルの文字コードがShift-JISになっている点に注意しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c75a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "unzip:  cannot find or open 794_ruby_4237.zip, 794_ruby_4237.zip.zip or 794_ruby_4237.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "# ファイルをダウンロードする\n",
    "!wget https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip\n",
    "# textフォルダ作る\n",
    "!mkdir -p text\n",
    "# ファイルをtextフォルダに解凍\n",
    "!unzip -d text -o 794_ruby_4237.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca434f",
   "metadata": {},
   "source": [
    "次に、正規表現で青空文庫のルビ、注、アクセントの記号を取り除きます。\n",
    "\n",
    "同時に、文字コードをShift-JISからUTF-8にします。\n",
    "\n",
    "```\n",
    "\n",
    "-------------------------------------------------------\n",
    "【テキスト中に現れる記号について】\n",
    "\n",
    "《》：ルビ\n",
    "（例）頓狂《とんきょう》\n",
    "\n",
    "｜：ルビの付く文字列の始まりを特定する記号\n",
    "（例）福岡県｜京都郡《みやこぐん》\n",
    "\n",
    "［＃］：入力者注　主に外字の説明や、傍点の位置の指定\n",
    "　　　（数字は、JIS X 0213の面区点番号またはUnicode、底本のページと行数）\n",
    "（例）※［＃「魚＋師のつくり」、第4水準2-93-37］\n",
    "\n",
    "〔〕：アクセント分解された欧文をかこむ\n",
    "（例）〔ve'rite'《ヴェリテ》 vraie《ヴレイ》.〕\n",
    "アクセント分解についての詳細は下記URLを参照してください\n",
    "http://www.aozora.gr.jp/accent_separation.html\n",
    "-------------------------------------------------------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be8d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_fn = \"text/sanshiro.txt\"\n",
    "output_fn = \"text/sanshiro.stripruby.txt\"\n",
    "\n",
    "with open(input_fn, encoding=\"shift_jis\") as fin, open(output_fn, mode=\"w\") as fout:\n",
    "    for line in fin:\n",
    "        fout.write(re.sub(r\"《[^》]+》|［[^］]+］|〔[^〕]+〕| [｜]\", \"\", line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94129c7b",
   "metadata": {},
   "source": [
    "冒頭と末尾の説明を取り除きます（何行取り除くかは目視で確認）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda308ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: brew\n",
      "zsh:1: command not found: ghead\n",
      "tail: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# macOSの場合は、homebrewでcoreutilsをインストールする必要があります（gheadを使うため）\n",
    "# homebrewがインストールされていない場合は、https://brew.sh/からインストールしてください\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    !brew install coreutils\n",
    "    !tail -n +22 text/sanshiro.stripruby.txt | ghead -n -14 > text/sanshiro.corpus.txt\n",
    "else:\n",
    "    !tail -n +22 text/sanshiro.stripruby.txt | head -n -14 > text/sanshiro.corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3b89d",
   "metadata": {},
   "source": [
    "これで、テキストファイルを扱う準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5840589",
   "metadata": {},
   "source": [
    "### ワードクラウド\n",
    "\n",
    "ワードクラウドは、形態素解析で得られた頻出単語の頻出度合いを文字の大きさで可視化する手法です。頻度の高い単語を大きく表示することで、テキスト全体の傾向を素早く理解することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a55698",
   "metadata": {},
   "source": [
    "#### 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb0eee",
   "metadata": {},
   "source": [
    "それでは、『三四郎』に出現する単語の頻度を数えてみましょう。\n",
    "\n",
    "テキストファイルを読み込んで形態素解析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bc7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = \"text/sanshiro.corpus.txt\"\n",
    "output_fn = \"text/sanshiro.wakati.txt\"\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "with open(input_fn, \"r\") as fin, open(output_fn, \"w\") as fout:\n",
    "    for line in fin:\n",
    "        tokens = [token.text for token in nlp(line.rstrip())]\n",
    "        fout.write(' '.join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5a14f",
   "metadata": {},
   "source": [
    "出力されたファイルを確認すると、単語分割されていることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e04dc",
   "metadata": {},
   "source": [
    "次に、使用頻度の高い単語を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35165aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルを読み込み、テキストを一行ずつ解析\n",
    "all_tokens = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        tokens = [token for token in nlp(line)]\n",
    "        all_tokens.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1bcc22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 分析対象とする品詞（内容語 - 名詞、動詞、形容詞）と不要語（ストップワード）を指定する\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "# 単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152e15c",
   "metadata": {},
   "source": [
    "#### ワードクラウドの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e8fb",
   "metadata": {},
   "source": [
    "それでは、このデータをもとにワードクラウドを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f22e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (from wordcloud) (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.4-cp311-cp311-macosx_10_9_x86_64.whl (171 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# ライブラリをインストールする\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ca7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語フォントのダウンロード（Linuxのみ）\n",
    "\n",
    "if sys.platform == \"linux\":\n",
    "    !sudo apt update\n",
    "    !sudo apt install fonts-ipaexfont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6230c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォントファイルの場所の指定（図が上手く表示されない場合は、書き換えてください）\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    fpath = \"/Library/Fonts/Arial Unicode.ttf\"\n",
    "else:\n",
    "    fpath = \"/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38faa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abaf93da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d98a1286",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ワードクラウドの表示設定と作成\u001b[39;00m\n\u001b[1;32m      5\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(\n\u001b[1;32m      6\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1600\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m,\n\u001b[1;32m      7\u001b[0m     background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m, font_path\u001b[38;5;241m=\u001b[39mfpath\n\u001b[0;32m----> 8\u001b[0m )\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# プロット\u001b[39;00m\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/wordcloud/wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_text(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/wordcloud/wordcloud.py:624\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(words)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/wordcloud/wordcloud.py:410\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    408\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(frequencies\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frequencies) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need at least 1 word to plot a word cloud, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(frequencies))\n\u001b[1;32m    412\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m frequencies[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_words]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# largest entry will be 1\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ワードクラウドの表示設定と作成\n",
    "wordcloud = WordCloud(\n",
    "    width=1600, height=800,\n",
    "    background_color=\"white\", font_path=fpath\n",
    ").generate(' '.join(words))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"wordcloud.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b22f7",
   "metadata": {},
   "source": [
    "「先生」、「言う」、「女」、「見る」などが高頻度で出現していることが分かります。\n",
    "\n",
    "分析対象の品詞に、固有名詞を加えてみたらどうでしょうか？試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57d298b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞と不要語を指定する\n",
    "# your code goes in ????? below\n",
    "\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"?????\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ca2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2ad7e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ワードクラウドの表示設定と作成\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(\n\u001b[1;32m      3\u001b[0m     width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1600\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m,\n\u001b[1;32m      4\u001b[0m     background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m, font_path\u001b[38;5;241m=\u001b[39mfpath\n\u001b[0;32m----> 5\u001b[0m )\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# プロット\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/wordcloud/wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_text(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/wordcloud/wordcloud.py:624\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(words)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/wordcloud/wordcloud.py:410\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    408\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(frequencies\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frequencies) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need at least 1 word to plot a word cloud, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(frequencies))\n\u001b[1;32m    412\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m frequencies[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_words]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# largest entry will be 1\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "# ワードクラウドの表示設定と作成\n",
    "wordcloud = WordCloud(\n",
    "    width=1600, height=800,\n",
    "    background_color=\"white\", font_path=fpath\n",
    ").generate(' '.join(words))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2909c",
   "metadata": {},
   "source": [
    "### 共起ネットワーク\n",
    "\n",
    "次に、共起ネットワークを作って、どの語とどの語が一緒に使われているかを調べてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b27f9a",
   "metadata": {},
   "source": [
    "#### 共起分析\n",
    "\n",
    "まず、共起分析を行います。文章を文に分割し、同一文中に同時に出現する単語の組を数え上げることで分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc555471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    \"\"\"\n",
    "    分析対象の品詞であり、不要語ではない単語を抽出する\n",
    "    \"\"\"\n",
    "    words = [token.lemma_ for token in sent if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cooccurrence(sents, token_length=\"{2,}\"):\n",
    "    \"\"\"\n",
    "    同じ文中に共起する単語を行列形式で列挙する\n",
    "    \"\"\"\n",
    "    token_pattern = f\"\\\\b\\\\w{token_length}\\\\b\"\n",
    "    count_model = CountVectorizer(token_pattern=token_pattern)\n",
    "\n",
    "    X = count_model.fit_transform(sents)\n",
    "    words = count_model.get_feature_names_out()\n",
    "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    X[X > 0] = 1 # 同じ共起が2以上出現しても1とする\n",
    "    Xc = (X.T * X) # 共起行列を求めるための掛け算をする、csr形式の疎行列\n",
    "\n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_by_cooccurrence(X, idxs):\n",
    "    \"\"\"\n",
    "    指定された共起を含む文を見つける\n",
    "    \"\"\"\n",
    "    occur_flags = (X[:, idxs[0]] > 0)\n",
    "    for idx in idxs[1:]:\n",
    "        occur_flags = occur_flags.multiply(X[:, idx] > 0)\n",
    "\n",
    "    return occur_flags.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を解析し、共起を求める\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "sents = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = nlp(line)\n",
    "        tmp = [' '.join(extract_words(sent, include_pos, stopwords)) for sent in doc.sents]\n",
    "        sents.extend(tmp)\n",
    "\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起ランキングを出力する\n",
    "# 共起行列Xcは疎行列なので、非ゼロ要素のみをカウンタに格納する\n",
    "counter = Counter()\n",
    "for i, j in zip(*Xc.nonzero()):\n",
    "    if i >= j:\n",
    "        continue\n",
    "    counter[(i, j)] += Xc[i, j]\n",
    "\n",
    "# 共起の出現頻度top 20を出力する\n",
    "for (i, j), c in counter.most_common(20):\n",
    "    print(f\"{c:>3d} ({words[i]}, {words[j]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定した共起を含む文のリストを出力する\n",
    "sents_orig = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = nlp(line)\n",
    "        tmp = list(doc.sents)\n",
    "        sents_orig.extend(tmp)\n",
    "\n",
    "# すべての単語の通し番号を求める\n",
    "words_lookup = { word: index for index, word in enumerate(words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起語を指定する\n",
    "lookup_words = [\"野々宮\", \"美禰子\"]\n",
    "\n",
    "# 指定した共起語のインデックスを求める\n",
    "idxs = list(map(lambda x: words_lookup[x], lookup_words))\n",
    "\n",
    "# 指定した共起を含む文のリストを出力する\n",
    "for i in find_sentence_by_cooccurrence(X, idxs):\n",
    "    print(f\"{i:>5d}: {sents_orig[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bef69",
   "metadata": {},
   "source": [
    "#### 共起ネットワークの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01164062",
   "metadata": {},
   "source": [
    "共起分析の結果に基づいて共起ネットワークを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx pyvis japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af921ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35721025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_weights(words, word_counts):\n",
    "    \"\"\"\n",
    "    単語の最多頻度が1となるような相対値として単語の重みを求める\n",
    "    \"\"\"\n",
    "    count_max = word_counts.max()\n",
    "    weights = [(word, {\"weight\": count / count_max})\n",
    "               for word, count in zip(words, word_counts)]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_weights(words, Xc, weight_cutoff):\n",
    "    \"\"\"\n",
    "    共起の最多頻度が1となるような相対値として共起の重みを求める\n",
    "    共起の重みがweight_cutoffより低い共起は除外する\n",
    "    \"\"\"\n",
    "    Xc_max = Xc.max()\n",
    "    cutoff = weight_cutoff * Xc_max\n",
    "    weights = [(words[i], words[j], Xc[i, j] / Xc_max)\n",
    "               for i, j in zip(*Xc.nonzero()) if i < j and Xc[i, j] > cutoff]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(words, word_counts, Xc, weight_cutoff):\n",
    "    \"\"\"\n",
    "    語、単語頻度、共起行列から共起ネットワークをNetworkX形式で得る\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "\n",
    "    weights_c = cooccurrence_weights(words, Xc, weight_cutoff)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyplot_network(G):\n",
    "    \"\"\"\n",
    "    NetworkX形式で与えられた共起ネットワークをpyplotで描画する\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, \"weight\").values()))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300 * weights_n)\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, \"weight\").values()))\n",
    "    nx.draw_networkx_edges(G, pos, width=20 * weights_e)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_family=\"IPAexGothic\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5066c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx2pyvis_G(G):\n",
    "    \"\"\"\n",
    "    NetworkX形式で与えられた共起ネットワークをpyvisで描画する\n",
    "    \"\"\"\n",
    "    pyvis_G = Network(width=\"800px\", height=\"800px\", notebook=True)\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pyvis_G.add_node(node, title=node, size=30 * attrs[\"weight\"])\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        pyvis_G.add_edge(node1, node2, width=20 * attrs[\"weight\"])\n",
    "\n",
    "    return pyvis_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c06913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークを作る\n",
    "G = create_network(words, word_counts, Xc, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca19a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 静的ビジュアライゼーション\n",
    "pyplot_network(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インタラクティブなビジュアライゼーション\n",
    "pyvis_G = nx2pyvis_G(G)\n",
    "pyvis_G.show_buttons()\n",
    "pyvis_G.show(\"network.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4ca48",
   "metadata": {},
   "source": [
    "このファイルを同じディレクトリに保存された`network.html`をブラウザで開き、インタラクティブな共起ネットワークを確認してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da92e0",
   "metadata": {},
   "source": [
    "### 共起ヒートマップ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf64940",
   "metadata": {},
   "source": [
    "共起関係をネットワークではなく、ヒートマップとして表現することもできます。\n",
    "\n",
    "登場人物の共起ヒートマップを作ってみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce90a36",
   "metadata": {},
   "source": [
    "#### データフレームを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を解析し、共起を求める\n",
    "include_pos = (\"NOUN\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "sents = []\n",
    "with open(input_fn, \"r\") as f:\n",
    "    for line in f:\n",
    "        doc = nlp(line)\n",
    "        tmp = [' '.join(extract_words(sent, include_pos, stopwords)) for sent in doc.sents]\n",
    "        sents.extend(tmp)\n",
    "\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46039f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列Xcは疎行列なので、非ゼロ要素のみをカウンタに格納する\n",
    "counter = Counter()\n",
    "for i, j in zip(*Xc.nonzero()):\n",
    "    if i >= j:\n",
    "        continue\n",
    "    counter[(i, j)] += Xc[i, j]\n",
    "\n",
    "# 共起の出現頻度top 20を出力する\n",
    "for (i, j), c in counter.most_common(20):\n",
    "    print(f\"{c:>3d} ({words[i]}, {words[j]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行名を用意する\n",
    "\n",
    "columns = set(Xc.nonzero()[0])\n",
    "columns_text = [words[i] for i in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42569a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべて0のデータフレームを用意する\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.zeros((len(columns), len(columns))), index=columns_text, columns=columns_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f488da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームに頻度を入れる\n",
    "\n",
    "for cord, count in counter.items():\n",
    "    df.iloc[cord] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登場人物名のみを取り出す\n",
    "\n",
    "characters = [\"三四郎\", \"広田\", \"野々宮\", \"佐々木\", \"与次郎\", \"美禰子\", \"先生\", \"原口\", \"里見\"]\n",
    "df_characters = df[characters].filter(items=characters, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c693323",
   "metadata": {},
   "source": [
    "#### 共起ヒートマップの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695614a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(df_characters, color_continuous_scale=px.colors.sequential.Oranges,\n",
    "                title=\"『三四郎』の登場人物\", width=800, height=800)\n",
    "fig.update_layout(font=dict(size=16))\n",
    "fig.show()\n",
    "fig.write_image(\"heatmap.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbca9a",
   "metadata": {},
   "source": [
    "人々のインタラクションの多寡が示されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca8e60",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303ca15",
   "metadata": {},
   "source": [
    "これらの可視化手法は文学作品だけではなく、他の分野のテキストデータにも有効です。\n",
    "\n",
    "文学作品に限っていうと、複数の作家の作品群を比較すると面白い結果が出るかもしれません。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
